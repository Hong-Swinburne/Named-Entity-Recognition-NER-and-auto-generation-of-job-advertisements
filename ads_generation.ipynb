{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto-generation of advertisements\n",
    "## Dependency\n",
    "**Note**: To run this notebook, the following packages should be installed\n",
    "- transformers\n",
    "- pandas\n",
    "- torch\n",
    "- scikit-learn\n",
    "\n",
    "I have successfully tested the notebook in Python 3.7+, it should also work in other Python3.x environment\n",
    "\n",
    "## Overview\n",
    "This notebook demonstrates a possible solution for generating job advertisement automatically with a pre-trained generative transformer model. Specially, this transformer model is a [GPT-2 model](https://openai.com/blog/better-language-models/) which is fine-tuned on the given meta information (e.g. job title, abstract, keyword etc.) related to the job ads.\n",
    "\n",
    "The training, validation and test corpus used in this use case are ads containing the words *machine learning*, *data science/scientist*, *AI* and *artificial intelligence* in the job description of provided JSON file. Note that due to the time limitation, I did not go through the whole 50,000 ads and specially pick all well-written and concise ads out of these ads. But this solution may be used to assist advertisers to automatically generate new better written, concise ads, if we carefully prepare such a training corpus to \"teach\" the model what well-written and concise ads look like.\n",
    "\n",
    "I followed [this blog](https://towardsdatascience.com/conditional-text-generation-by-fine-tuning-gpt-2-11c1a9fc639d) to fine-tune the pre-trained GPT-2 on our own ads dataset. For more technical details, please refer to [\"Conditional Text Generation by Fine Tuning GPT-2\"](https://towardsdatascience.com/conditional-text-generation-by-fine-tuning-gpt-2-11c1a9fc639d)\n",
    "\n",
    "A quick look of the pipeline contained in this study case\n",
    "\n",
    "<ol>\n",
    "    <li>Prepare text corpus\n",
    "        <ul>\n",
    "            <li> Remove columns irrelevant to the task\n",
    "            <li> Check whether the data contain missing values in \"title\", \"abstract\", \"clean_text\" etc.\n",
    "        </ul>\n",
    "    <li> Configuration of hyper-parameters\n",
    "         <ul>\n",
    "             <li> Configure hyper-parameters for dataset split and model training\n",
    "             <li> Fix seeds used in dataset generation and model initialization\n",
    "        </ul>\n",
    "    <li> Load tokenizer\n",
    "    <li> Create training and validation dataset\n",
    "        <ul>\n",
    "             <li> Define Dataset class for dataset generation\n",
    "             <li> Split whole dataset into training and validation sets\n",
    "        </ul>\n",
    "    <li> Load language model\n",
    "    <li> Fine-tuning of language model\n",
    "        <ul>\n",
    "             <li> Freeze the corresponding layers in language model\n",
    "             <li> Fine tune the language model on our dataset\n",
    "             <li> Save the fine-tuned model\n",
    "        </ul>\n",
    "    <li> Advertisement generation using the fine-tuned model\n",
    "        <ul>\n",
    "             <li> Load tokenizer and fine-tuned model\n",
    "             <li> Generate prompt for ad generation\n",
    "             <li> Ad generation using greedy search\n",
    "             <li> Ad generation using beam search\n",
    "        </ul>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoConfig, AutoModelForPreTraining, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare text corpus\n",
    "\n",
    "We selected the job ads that contains the words such as *machine learning*, *data science/scientist*, *AI* and *artificial intelligence* in the job description to form the text corpus that was used to fine-tune the pretrained GPT-2 language model for job ads generation. The aim of the fine-tuned job ads generator is to input the job title, job abstract (1-2 sentences), and some keywords(e.g. skills, responsibilities etc.) of the job description, the generator, ideally, would create the whole job advertisement with responsibility, skills required, and other relevant and detailed information.\n",
    "\n",
    "To prepare the text corpus for fine-tuning the language model, we load the data frame, and only keep \"id\", \"quality\", \"title\", \"abstract\", \"clean_text\", \"skills\", \"responsilibities\" etc. (detailed job contents after preprocessing e.g. removal of HTML tags ) columns and remove other columns in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "find 373 examples\n"
     ]
    }
   ],
   "source": [
    "df_ads = pd.read_csv('JD_ML.csv', encoding='windows-1252')\n",
    "remove_cols = [col for col in df_ads.columns if col not in [\"id\", \"quality\", \"title\", \"abstract\", \"clean_text\", \"SKILLS\", \"RESPONSIBILITIES\", \"REQUIREMENTS\", \"EXPERIENCE\", \"QUALIFICATION\"]]\n",
    "df_ads.drop(columns=remove_cols, inplace=True)\n",
    "print(f'find {len(df_ads)} examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>quality</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>SKILLS</th>\n",
       "      <th>RESPONSIBILITIES</th>\n",
       "      <th>REQUIREMENTS</th>\n",
       "      <th>EXPERIENCE</th>\n",
       "      <th>QUALIFICATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38964460</td>\n",
       "      <td>bad</td>\n",
       "      <td>Senior Product Manager</td>\n",
       "      <td>Electronic Arts is looking for a full time Sen...</td>\n",
       "      <td>The Company We are EA! And we make games ????...</td>\n",
       "      <td>strong quantitative background to create, bala...</td>\n",
       "      <td>Measure performance and adjust\\nWork closely w...</td>\n",
       "      <td>designing the game economy, and providing anal...</td>\n",
       "      <td>experience with data and metrics driven decisi...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38996736</td>\n",
       "      <td>good</td>\n",
       "      <td>Social Media/membership and Events Assistant</td>\n",
       "      <td>An ideal entry level role initially working on...</td>\n",
       "      <td>Management of AIPS Website including: Ensurin...</td>\n",
       "      <td>Quarterly in managing of AIPS public Facebook/...</td>\n",
       "      <td>Ensuring all information is current and releva...</td>\n",
       "      <td>ability to increase the frequency and reach of...</td>\n",
       "      <td>8 - 12 hrs hours per week, flexible hours and ...</td>\n",
       "      <td>Office management skills\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38819733</td>\n",
       "      <td>bad</td>\n",
       "      <td>Pricing Analyst's all levels, Sydney CBD</td>\n",
       "      <td>Superb roles with leading Financial Services f...</td>\n",
       "      <td>My client is a Financial Services organisatio...</td>\n",
       "      <td>SQL\\nSAS\\nPython\\nability in the fullness of t...</td>\n",
       "      <td>working as part of a customer-facing team who ...</td>\n",
       "      <td>Candidates also need a solid understanding of ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38979340</td>\n",
       "      <td>good</td>\n",
       "      <td>IT Designer</td>\n",
       "      <td>Our client is seeking an experienced IT Design...</td>\n",
       "      <td>Our client is seeking an experienced IT Desig...</td>\n",
       "      <td>Agile delivery\\nshare knowledge and informatio...</td>\n",
       "      <td>Analyse system, development or support issues ...</td>\n",
       "      <td>Interpret business requirements into applicati...</td>\n",
       "      <td>experience in working in a scaled agile enviro...</td>\n",
       "      <td>Australian Citizens\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38912123</td>\n",
       "      <td>good</td>\n",
       "      <td>Project Manager - Service Delivery</td>\n",
       "      <td>5 years of experience working in managing end ...</td>\n",
       "      <td>Infosys is a global leader in next-generation...</td>\n",
       "      <td>able to communicate by telephone, email\\n</td>\n",
       "      <td>creating new avenues to generate value\\nCommun...</td>\n",
       "      <td>Location in Australia\\nAbility to work in team...</td>\n",
       "      <td>5 years of experience working in managing end ...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>38959448</td>\n",
       "      <td>good</td>\n",
       "      <td>Aged Care Nurses. RN's, EN's &amp; Cert 3 AIN/PCA'...</td>\n",
       "      <td>RNS Nursing is seeking experienced Aged Care (...</td>\n",
       "      <td>JOB ROLE We are seeking experienced, compassi...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Roster update availability Friendly supportive...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Cert 3 AIN/PCA's to provide a high quality of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38926745</td>\n",
       "      <td>good</td>\n",
       "      <td>NDT Technician</td>\n",
       "      <td>APTS are urgently seeking an experienced NDT T...</td>\n",
       "      <td>APTS Pty Ltd provides service excellence in t...</td>\n",
       "      <td>ability to commence immediately\\nInterpret and...</td>\n",
       "      <td>Mining, Power and Water Treatment\\nSet up and ...</td>\n",
       "      <td>Perform and supervise tests\\n</td>\n",
       "      <td>adaptable with working hours which can vary fr...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>38847053</td>\n",
       "      <td>good</td>\n",
       "      <td>Agile Business Analyst</td>\n",
       "      <td>12 + 24 month Engagement.  Lead a high perform...</td>\n",
       "      <td>Our Federal Government client requires an exp...</td>\n",
       "      <td>ability to work effectively with multiple disc...</td>\n",
       "      <td>Engage with customers and stakeholders to unde...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Experience working as a Business Analyst in an...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>38856874</td>\n",
       "      <td>bad</td>\n",
       "      <td>Data Analysts</td>\n",
       "      <td>Aspiring Data gurus? Do you have a passion for...</td>\n",
       "      <td>Client We are currently supporting a global f...</td>\n",
       "      <td>Strong business analysis skills\\nTableau\\nUnde...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Following a detailed review of their organisat...</td>\n",
       "      <td>At least two years working within a data analy...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>38823908</td>\n",
       "      <td>good</td>\n",
       "      <td>Personal Carer</td>\n",
       "      <td>Personal carer position available for Monday t...</td>\n",
       "      <td>DO YOU SHARE A VISION OF EXCELLENCE IN PERSON...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>provide the help or assistance required to ens...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id quality                                              title  \\\n",
       "0  38964460     bad                             Senior Product Manager   \n",
       "1  38996736    good       Social Media/membership and Events Assistant   \n",
       "2  38819733     bad           Pricing Analyst's all levels, Sydney CBD   \n",
       "3  38979340    good                                        IT Designer   \n",
       "4  38912123    good                 Project Manager - Service Delivery   \n",
       "5  38959448    good  Aged Care Nurses. RN's, EN's & Cert 3 AIN/PCA'...   \n",
       "6  38926745    good                                     NDT Technician   \n",
       "7  38847053    good                             Agile Business Analyst   \n",
       "8  38856874     bad                                      Data Analysts   \n",
       "9  38823908    good                                     Personal Carer   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Electronic Arts is looking for a full time Sen...   \n",
       "1  An ideal entry level role initially working on...   \n",
       "2  Superb roles with leading Financial Services f...   \n",
       "3  Our client is seeking an experienced IT Design...   \n",
       "4  5 years of experience working in managing end ...   \n",
       "5  RNS Nursing is seeking experienced Aged Care (...   \n",
       "6  APTS are urgently seeking an experienced NDT T...   \n",
       "7  12 + 24 month Engagement.  Lead a high perform...   \n",
       "8  Aspiring Data gurus? Do you have a passion for...   \n",
       "9  Personal carer position available for Monday t...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0   The Company We are EA! And we make games ????...   \n",
       "1   Management of AIPS Website including: Ensurin...   \n",
       "2   My client is a Financial Services organisatio...   \n",
       "3   Our client is seeking an experienced IT Desig...   \n",
       "4   Infosys is a global leader in next-generation...   \n",
       "5   JOB ROLE We are seeking experienced, compassi...   \n",
       "6   APTS Pty Ltd provides service excellence in t...   \n",
       "7   Our Federal Government client requires an exp...   \n",
       "8   Client We are currently supporting a global f...   \n",
       "9   DO YOU SHARE A VISION OF EXCELLENCE IN PERSON...   \n",
       "\n",
       "                                              SKILLS  \\\n",
       "0  strong quantitative background to create, bala...   \n",
       "1  Quarterly in managing of AIPS public Facebook/...   \n",
       "2  SQL\\nSAS\\nPython\\nability in the fullness of t...   \n",
       "3  Agile delivery\\nshare knowledge and informatio...   \n",
       "4          able to communicate by telephone, email\\n   \n",
       "5                                                NaN   \n",
       "6  ability to commence immediately\\nInterpret and...   \n",
       "7  ability to work effectively with multiple disc...   \n",
       "8  Strong business analysis skills\\nTableau\\nUnde...   \n",
       "9                                                NaN   \n",
       "\n",
       "                                    RESPONSIBILITIES  \\\n",
       "0  Measure performance and adjust\\nWork closely w...   \n",
       "1  Ensuring all information is current and releva...   \n",
       "2  working as part of a customer-facing team who ...   \n",
       "3  Analyse system, development or support issues ...   \n",
       "4  creating new avenues to generate value\\nCommun...   \n",
       "5  Roster update availability Friendly supportive...   \n",
       "6  Mining, Power and Water Treatment\\nSet up and ...   \n",
       "7  Engage with customers and stakeholders to unde...   \n",
       "8                                                NaN   \n",
       "9  provide the help or assistance required to ens...   \n",
       "\n",
       "                                        REQUIREMENTS  \\\n",
       "0  designing the game economy, and providing anal...   \n",
       "1  ability to increase the frequency and reach of...   \n",
       "2  Candidates also need a solid understanding of ...   \n",
       "3  Interpret business requirements into applicati...   \n",
       "4  Location in Australia\\nAbility to work in team...   \n",
       "5                                                NaN   \n",
       "6                      Perform and supervise tests\\n   \n",
       "7                                                NaN   \n",
       "8  Following a detailed review of their organisat...   \n",
       "9                                                NaN   \n",
       "\n",
       "                                          EXPERIENCE  \\\n",
       "0  experience with data and metrics driven decisi...   \n",
       "1  8 - 12 hrs hours per week, flexible hours and ...   \n",
       "2                                                NaN   \n",
       "3  experience in working in a scaled agile enviro...   \n",
       "4  5 years of experience working in managing end ...   \n",
       "5                                                NaN   \n",
       "6  adaptable with working hours which can vary fr...   \n",
       "7  Experience working as a Business Analyst in an...   \n",
       "8  At least two years working within a data analy...   \n",
       "9                                                NaN   \n",
       "\n",
       "                                       QUALIFICATION  \n",
       "0                                                NaN  \n",
       "1                         Office management skills\\n  \n",
       "2                                                NaN  \n",
       "3                              Australian Citizens\\n  \n",
       "4                                                NaN  \n",
       "5  Cert 3 AIN/PCA's to provide a high quality of ...  \n",
       "6                                                NaN  \n",
       "7                                                NaN  \n",
       "8                                                NaN  \n",
       "9                                                NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ads.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine job skills, responsibilities, requirements, experience, qualification into a new column \"keywords\", which will be input to the generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>quality</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38964460</td>\n",
       "      <td>bad</td>\n",
       "      <td>Senior Product Manager</td>\n",
       "      <td>Electronic Arts is looking for a full time Sen...</td>\n",
       "      <td>The Company We are EA! And we make games ????...</td>\n",
       "      <td>strong quantitative background to create, bala...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38996736</td>\n",
       "      <td>good</td>\n",
       "      <td>Social Media/membership and Events Assistant</td>\n",
       "      <td>An ideal entry level role initially working on...</td>\n",
       "      <td>Management of AIPS Website including: Ensurin...</td>\n",
       "      <td>Quarterly in managing of AIPS public Facebook/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38819733</td>\n",
       "      <td>bad</td>\n",
       "      <td>Pricing Analyst's all levels, Sydney CBD</td>\n",
       "      <td>Superb roles with leading Financial Services f...</td>\n",
       "      <td>My client is a Financial Services organisatio...</td>\n",
       "      <td>SQL\\nSAS\\nPython\\nability in the fullness of t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38979340</td>\n",
       "      <td>good</td>\n",
       "      <td>IT Designer</td>\n",
       "      <td>Our client is seeking an experienced IT Design...</td>\n",
       "      <td>Our client is seeking an experienced IT Desig...</td>\n",
       "      <td>Agile delivery\\nshare knowledge and informatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38912123</td>\n",
       "      <td>good</td>\n",
       "      <td>Project Manager - Service Delivery</td>\n",
       "      <td>5 years of experience working in managing end ...</td>\n",
       "      <td>Infosys is a global leader in next-generation...</td>\n",
       "      <td>able to communicate by telephone, email\\n\\ncre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>38959448</td>\n",
       "      <td>good</td>\n",
       "      <td>Aged Care Nurses. RN's, EN's &amp; Cert 3 AIN/PCA'...</td>\n",
       "      <td>RNS Nursing is seeking experienced Aged Care (...</td>\n",
       "      <td>JOB ROLE We are seeking experienced, compassi...</td>\n",
       "      <td>Roster update availability Friendly supportive...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>38926745</td>\n",
       "      <td>good</td>\n",
       "      <td>NDT Technician</td>\n",
       "      <td>APTS are urgently seeking an experienced NDT T...</td>\n",
       "      <td>APTS Pty Ltd provides service excellence in t...</td>\n",
       "      <td>ability to commence immediately\\nInterpret and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>38847053</td>\n",
       "      <td>good</td>\n",
       "      <td>Agile Business Analyst</td>\n",
       "      <td>12 + 24 month Engagement.  Lead a high perform...</td>\n",
       "      <td>Our Federal Government client requires an exp...</td>\n",
       "      <td>ability to work effectively with multiple disc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>38856874</td>\n",
       "      <td>bad</td>\n",
       "      <td>Data Analysts</td>\n",
       "      <td>Aspiring Data gurus? Do you have a passion for...</td>\n",
       "      <td>Client We are currently supporting a global f...</td>\n",
       "      <td>Strong business analysis skills\\nTableau\\nUnde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>38823908</td>\n",
       "      <td>good</td>\n",
       "      <td>Personal Carer</td>\n",
       "      <td>Personal carer position available for Monday t...</td>\n",
       "      <td>DO YOU SHARE A VISION OF EXCELLENCE IN PERSON...</td>\n",
       "      <td>provide the help or assistance required to ens...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id quality                                              title  \\\n",
       "0  38964460     bad                             Senior Product Manager   \n",
       "1  38996736    good       Social Media/membership and Events Assistant   \n",
       "2  38819733     bad           Pricing Analyst's all levels, Sydney CBD   \n",
       "3  38979340    good                                        IT Designer   \n",
       "4  38912123    good                 Project Manager - Service Delivery   \n",
       "5  38959448    good  Aged Care Nurses. RN's, EN's & Cert 3 AIN/PCA'...   \n",
       "6  38926745    good                                     NDT Technician   \n",
       "7  38847053    good                             Agile Business Analyst   \n",
       "8  38856874     bad                                      Data Analysts   \n",
       "9  38823908    good                                     Personal Carer   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Electronic Arts is looking for a full time Sen...   \n",
       "1  An ideal entry level role initially working on...   \n",
       "2  Superb roles with leading Financial Services f...   \n",
       "3  Our client is seeking an experienced IT Design...   \n",
       "4  5 years of experience working in managing end ...   \n",
       "5  RNS Nursing is seeking experienced Aged Care (...   \n",
       "6  APTS are urgently seeking an experienced NDT T...   \n",
       "7  12 + 24 month Engagement.  Lead a high perform...   \n",
       "8  Aspiring Data gurus? Do you have a passion for...   \n",
       "9  Personal carer position available for Monday t...   \n",
       "\n",
       "                                          clean_text  \\\n",
       "0   The Company We are EA! And we make games ????...   \n",
       "1   Management of AIPS Website including: Ensurin...   \n",
       "2   My client is a Financial Services organisatio...   \n",
       "3   Our client is seeking an experienced IT Desig...   \n",
       "4   Infosys is a global leader in next-generation...   \n",
       "5   JOB ROLE We are seeking experienced, compassi...   \n",
       "6   APTS Pty Ltd provides service excellence in t...   \n",
       "7   Our Federal Government client requires an exp...   \n",
       "8   Client We are currently supporting a global f...   \n",
       "9   DO YOU SHARE A VISION OF EXCELLENCE IN PERSON...   \n",
       "\n",
       "                                            keywords  \n",
       "0  strong quantitative background to create, bala...  \n",
       "1  Quarterly in managing of AIPS public Facebook/...  \n",
       "2  SQL\\nSAS\\nPython\\nability in the fullness of t...  \n",
       "3  Agile delivery\\nshare knowledge and informatio...  \n",
       "4  able to communicate by telephone, email\\n\\ncre...  \n",
       "5  Roster update availability Friendly supportive...  \n",
       "6  ability to commence immediately\\nInterpret and...  \n",
       "7  ability to work effectively with multiple disc...  \n",
       "8  Strong business analysis skills\\nTableau\\nUnde...  \n",
       "9  provide the help or assistance required to ens...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_ads[\"keywords\"] = df_ads[df_ads.columns[5:]].apply(lambda x: '\\n'.join(x.dropna()),  axis=1)\n",
    "df_ads[\"keywords\"] = df_ads[df_ads.columns[5:7]].apply(lambda x: '\\n'.join(x.dropna()),  axis=1)\n",
    "df_ads.drop(columns=[\"SKILLS\", \"RESPONSIBILITIES\", \"REQUIREMENTS\", \"EXPERIENCE\", \"QUALIFICATION\"], inplace=True)\n",
    "df_ads.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if the dataframe contain missing values in \"conciseness\", \"title\", \"abstract\", \"clean_text\", \"keywords\" etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 373 entries, 0 to 372\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   id          373 non-null    int64 \n",
      " 1   quality     373 non-null    object\n",
      " 2   title       373 non-null    object\n",
      " 3   abstract    373 non-null    object\n",
      " 4   clean_text  373 non-null    object\n",
      " 5   keywords    373 non-null    object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 17.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# check if the dataframe has missing values \n",
    "df_ads.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As all columns contain non-null objects,  we do not need to handle the missing value. Take a look at the number of concise and non-concise ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "good    209\n",
       "bad     164\n",
       "Name: quality, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of good-quality and bad-quality ads in the dataframe\n",
    "df_ads[\"quality\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select concise ads for fine-tuning the generative model and non-concise ads for testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_good_ads = df_ads.loc[df_ads[\"quality\"]==\"good\"]\n",
    "df_bad_ads = df_ads.loc[df_ads[\"quality\"]==\"bad\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration of hyper-parameters\n",
    "\n",
    "Next we set up some hyper-parameters that will be used later for dataset split, tokenizer generation and model initialization & training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Configure hyper-parameters for training, parameters are referred to https://colab.research.google.com/drive/16UTbQOhspQOF3XlxDFyI28S-0nAkTzk_#scrollTo=vCPohrZ-CTWu\n",
    "MODELS = [\"gpt2\", \"gpt2-medium\", \"gpt2-large\", \"gpt2-xl\"]\n",
    "MODEL = MODELS[0]\n",
    "\n",
    "USE_APEX = True # Enable mixed precision in training\n",
    "APEX_OPT_LEVEL  = 'O1'\n",
    "UNFREEZE_LAST_N = 6 # Unfreeze the last N layers in GPT-2 model for fine-tuning\n",
    "\n",
    "SPECIAL_TOKENS  = { \"bos_token\": \"<|BOS|>\",\n",
    "                    \"eos_token\": \"<|EOS|>\",\n",
    "                    \"unk_token\": \"<|UNK|>\",                    \n",
    "                    \"pad_token\": \"<|PAD|>\",\n",
    "                    \"sep_token\": \"<|SEP|>\"}\n",
    "                    \n",
    "MAXLEN = 768  #{768, 1024, 1280, 1600} # Maximum number of tokens can be used in GPT-2 model\n",
    "\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "if USE_APEX:\n",
    "    BATCHSIZE = 3\n",
    "    BATCH_UPDATE = 16\n",
    "else:\n",
    "    BATCHSIZE = 2\n",
    "    BATCH_UPDATE = 32\n",
    "\n",
    "EPOCHS = 20\n",
    "LR = 5e-4\n",
    "EPS  = 1e-8\n",
    "WARMUP_STEPS = 1e2\n",
    "WD = 0.01\n",
    "\n",
    "SEED = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the reproducible result, we fixed the seed values for dataset generation and model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load tokenizer\n",
    "\n",
    "Load the default GPT-2 tokenizer and add some special tokens such as *begin of text*(**\"<|BOS|>\"**), *end of text*(**\"<|EOS|>\"**), *unknown word*(**\"<|UNK|>\"**), *padding token*(**\"<|PAD|>\"**), and *separate token*(**\"<|SEP|>\"**) to the tokenizer. The tokenizer will be used to encode the input text into vector representation for model training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens added\n"
     ]
    }
   ],
   "source": [
    "# load tokenizer\n",
    "def get_tokenizer(special_tokens=None, load_token_path=None):\n",
    "    \n",
    "    if load_token_path:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(load_token_path)\n",
    "    \n",
    "    else:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL) #GPT2Tokenizer\n",
    "        if special_tokens:\n",
    "            tokenizer.add_special_tokens(special_tokens)\n",
    "            print(\"Special tokens added\")\n",
    "    \n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(special_tokens=SPECIAL_TOKENS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create training and validation dataset\n",
    "\n",
    "### Define Dataset class for dataset generation\n",
    "\n",
    "For each job ad, we join the job title, job abstract and job content sequentially into an input text and encode it into vectors using the GPT-2 tokenizer. The encoded vectors are then fed into the language model for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df_data, tokenizer, randomize=True):\n",
    "        self.randomize = randomize\n",
    "        self.tokenizer = tokenizer \n",
    "        self.title     = df_data[\"title\"].tolist()\n",
    "        self.text      = df_data[\"clean_text\"].tolist()\n",
    "        self.abstract  = df_data[\"abstract\"].tolist()\n",
    "        self.keywords  = df_data[\"keywords\"].tolist()\n",
    "     \n",
    "    @staticmethod\n",
    "    def join_keywords(keywords, randomize=True):\n",
    "        \n",
    "        kws = keywords.replace('\\n\\n', '\\n').splitlines()\n",
    "        N = len(kws)\n",
    "\n",
    "        #random sampling and shuffle\n",
    "        if randomize: \n",
    "            M = random.choice(range(N+1))\n",
    "            kws = kws[:M]\n",
    "            random.shuffle(kws)\n",
    "\n",
    "        return ';'.join(kws)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.title)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        keywords = self.keywords[idx]\n",
    "\n",
    "        kw = self.join_keywords(keywords, self.randomize)\n",
    "\n",
    "        \"\"\"\n",
    "        For ad content in each Json record, we prepend it with the start of text token, the title, abstract of that ad,\n",
    "        then append it with the end of text token and pad to the maximum length with the pad token. \n",
    "        \"\"\"\n",
    "        input = SPECIAL_TOKENS['bos_token'] + self.title[idx] + \\\n",
    "                SPECIAL_TOKENS['sep_token'] + self.abstract[idx] + \\\n",
    "                SPECIAL_TOKENS['sep_token'] + kw + SPECIAL_TOKENS['sep_token'] + \\\n",
    "                self.text[idx] + SPECIAL_TOKENS['eos_token']\n",
    "            \n",
    "        # input = SPECIAL_TOKENS['bos_token'] + self.title[idx] + SPECIAL_TOKENS['sep_token'] + \\\n",
    "        #         self.abstract[idx] + SPECIAL_TOKENS['sep_token'] + \\\n",
    "        #         self.text[idx] + SPECIAL_TOKENS['eos_token']\n",
    "\n",
    "        encodings_dict = tokenizer(input, truncation=True, max_length=MAXLEN, padding=\"max_length\")\n",
    "\n",
    "        input_ids = encodings_dict['input_ids']\n",
    "        attention_mask = encodings_dict['attention_mask']\n",
    "\n",
    "        \"\"\"\n",
    "        Appends both the encoded tensor and the attention mask for that encoding to a dict. The attention mask is\n",
    "        a binary list of 1's or 0's which determine whether the langauge model should take that token into consideration or not. \n",
    "        \"\"\"\n",
    "\n",
    "        return {'label': torch.tensor(input_ids),\n",
    "                'input_ids': torch.tensor(input_ids), \n",
    "                'attention_mask': torch.tensor(attention_mask)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split good-quality ads into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "167 samples for training, and 42 samples for validation\n",
      "164 samples for testing\n"
     ]
    }
   ],
   "source": [
    "# Split dataframe into training and validation data\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(df_good_ads, train_size=TRAIN_SIZE, random_state=SEED)\n",
    "print(f'{len(train_df)} samples for training, and {len(val_df)} samples for validation')\n",
    "\n",
    "test_df = df_bad_ads\n",
    "print(f'{len(test_df)} samples for testing')\n",
    "\n",
    "# shuffle training dataframes\n",
    "train_df.sample(frac=1, random_state=SEED)\n",
    "train_dataset = ADDataset(train_df, tokenizer)\n",
    "val_dataset = ADDataset(val_df, tokenizer, randomize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the language model\n",
    "\n",
    "Load and set the parameters of the GPT-2 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model successfully\n"
     ]
    }
   ],
   "source": [
    "# Load configuration and model \n",
    "def get_model(tokenizer, special_tokens=None, load_model_path=None):\n",
    "    \n",
    "    #GPT2LMHeadModel\n",
    "    if special_tokens:\n",
    "        config = AutoConfig.from_pretrained(MODEL, \n",
    "                                            bos_token_id=tokenizer.bos_token_id,\n",
    "                                            eos_token_id=tokenizer.eos_token_id,\n",
    "                                            sep_token_id=tokenizer.sep_token_id,\n",
    "                                            pad_token_id=tokenizer.pad_token_id,\n",
    "                                            output_hidden_states=False)\n",
    "    else: \n",
    "        config = AutoConfig.from_pretrained(MODEL,                                     \n",
    "                                            pad_token_id=tokenizer.eos_token_id,\n",
    "                                            output_hidden_states=False)    \n",
    "\n",
    "    model = AutoModelForPreTraining.from_pretrained(MODEL, config=config)\n",
    "\n",
    "    if special_tokens:\n",
    "        #Special tokens added, model needs to be resized accordingly\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    if load_model_path:\n",
    "        model.load_state_dict(torch.load(load_model_path))\n",
    "\n",
    "    # Run model on GPU\n",
    "    device = torch.device(\"cuda\")\n",
    "    model.cuda()\n",
    "    model = model.to(device)\n",
    "    return model\n",
    "\n",
    "model = get_model(tokenizer, special_tokens=SPECIAL_TOKENS)\n",
    "print(\"Load model successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune the language model\n",
    "\n",
    "Refer to [\"Conditional Text Generation by Fine Tuning GPT-2\"](\"https://towardsdatascience.com/conditional-text-generation-by-fine-tuning-gpt-2-11c1a9fc639d\"), we fine-tune the weights of the last 6 layers in GPT-2 model by setting its ```parameter.required_grad``` as ```True```, and freeze the other layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze the last N layers\n",
    "for parameter in model.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "for i, m in enumerate(model.transformer.h):        \n",
    "    # Only un-freeze the last n transformer blocks\n",
    "    if i+1 > 12 - UNFREEZE_LAST_N:\n",
    "        for parameter in m.parameters():\n",
    "            parameter.requires_grad = True \n",
    "\n",
    "for parameter in model.transformer.ln_f.parameters():        \n",
    "    parameter.requires_grad = True\n",
    "\n",
    "for parameter in model.lm_head.parameters():        \n",
    "    parameter.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune the language model on advertisement dataset\n",
    "\n",
    "Fine-tune the GPT-2 model on our own advertisement dataset using the pre-defined hyper-parameter values and the model configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n",
      "***** Running training *****\n",
      "  Num examples = 167\n",
      "  Num Epochs = 20\n",
      "  Instantaneous batch size per device = 3\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 60\n",
      "/fred/oz144/anaconda/envs/nlp/lib/python3.7/site-packages/transformers/trainer.py:1316: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 09:20, Epoch 19/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>No log</td>\n",
       "      <td>65.833359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>65.138977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>59.509144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>54.178036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>33.133171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>13.299901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.328084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.623007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.115868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.838604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.959438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.103912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.550911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.190593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.131158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.078454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.048890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.037457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.994887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.965510</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./output/ad_generation/gpt2/checkpoint-3\n",
      "Configuration saved in ./output/ad_generation/gpt2/checkpoint-3/config.json\n",
      "Model weights saved in ./output/ad_generation/gpt2/checkpoint-3/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ad_generation/gpt2/checkpoint-3/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ad_generation/gpt2/checkpoint-3/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ad_generation/gpt2/checkpoint-57] due to args.save_total_limit\n",
      "/fred/oz144/anaconda/envs/nlp/lib/python3.7/site-packages/transformers/trainer.py:1316: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  args.max_grad_norm,\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./output/ad_generation/gpt2/checkpoint-6\n",
      "Configuration saved in ./output/ad_generation/gpt2/checkpoint-6/config.json\n",
      "Model weights saved in ./output/ad_generation/gpt2/checkpoint-6/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ad_generation/gpt2/checkpoint-6/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ad_generation/gpt2/checkpoint-6/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ad_generation/gpt2/checkpoint-60] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./output/ad_generation/gpt2/checkpoint-9\n",
      "Configuration saved in ./output/ad_generation/gpt2/checkpoint-9/config.json\n",
      "Model weights saved in ./output/ad_generation/gpt2/checkpoint-9/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ad_generation/gpt2/checkpoint-9/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ad_generation/gpt2/checkpoint-9/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ad_generation/gpt2/checkpoint-3] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./output/ad_generation/gpt2/checkpoint-12\n",
      "Configuration saved in ./output/ad_generation/gpt2/checkpoint-12/config.json\n",
      "Model weights saved in ./output/ad_generation/gpt2/checkpoint-12/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ad_generation/gpt2/checkpoint-12/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ad_generation/gpt2/checkpoint-12/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ad_generation/gpt2/checkpoint-6] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./output/ad_generation/gpt2/checkpoint-15\n",
      "Configuration saved in ./output/ad_generation/gpt2/checkpoint-15/config.json\n",
      "Model weights saved in ./output/ad_generation/gpt2/checkpoint-15/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ad_generation/gpt2/checkpoint-15/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ad_generation/gpt2/checkpoint-15/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ad_generation/gpt2/checkpoint-9] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./output/ad_generation/gpt2/checkpoint-18\n",
      "Configuration saved in ./output/ad_generation/gpt2/checkpoint-18/config.json\n",
      "Model weights saved in ./output/ad_generation/gpt2/checkpoint-18/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ad_generation/gpt2/checkpoint-18/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ad_generation/gpt2/checkpoint-18/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ad_generation/gpt2/checkpoint-12] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./output/ad_generation/gpt2/checkpoint-21\n",
      "Configuration saved in ./output/ad_generation/gpt2/checkpoint-21/config.json\n",
      "Model weights saved in ./output/ad_generation/gpt2/checkpoint-21/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ad_generation/gpt2/checkpoint-21/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ad_generation/gpt2/checkpoint-21/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ad_generation/gpt2/checkpoint-15] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./output/ad_generation/gpt2/checkpoint-24\n",
      "Configuration saved in ./output/ad_generation/gpt2/checkpoint-24/config.json\n",
      "Model weights saved in ./output/ad_generation/gpt2/checkpoint-24/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ad_generation/gpt2/checkpoint-24/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ad_generation/gpt2/checkpoint-24/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ad_generation/gpt2/checkpoint-18] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./output/ad_generation/gpt2/checkpoint-27\n",
      "Configuration saved in ./output/ad_generation/gpt2/checkpoint-27/config.json\n",
      "Model weights saved in ./output/ad_generation/gpt2/checkpoint-27/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ad_generation/gpt2/checkpoint-27/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ad_generation/gpt2/checkpoint-27/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ad_generation/gpt2/checkpoint-21] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./output/ad_generation/gpt2/checkpoint-30\n",
      "Configuration saved in ./output/ad_generation/gpt2/checkpoint-30/config.json\n",
      "Model weights saved in ./output/ad_generation/gpt2/checkpoint-30/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ad_generation/gpt2/checkpoint-30/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ad_generation/gpt2/checkpoint-30/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ad_generation/gpt2/checkpoint-24] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./output/ad_generation/gpt2/checkpoint-33\n",
      "Configuration saved in ./output/ad_generation/gpt2/checkpoint-33/config.json\n",
      "Model weights saved in ./output/ad_generation/gpt2/checkpoint-33/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ad_generation/gpt2/checkpoint-33/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ad_generation/gpt2/checkpoint-33/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ad_generation/gpt2/checkpoint-27] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./output/ad_generation/gpt2/checkpoint-36\n",
      "Configuration saved in ./output/ad_generation/gpt2/checkpoint-36/config.json\n",
      "Model weights saved in ./output/ad_generation/gpt2/checkpoint-36/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ad_generation/gpt2/checkpoint-36/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ad_generation/gpt2/checkpoint-36/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ad_generation/gpt2/checkpoint-33] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./output/ad_generation/gpt2/checkpoint-39\n",
      "Configuration saved in ./output/ad_generation/gpt2/checkpoint-39/config.json\n",
      "Model weights saved in ./output/ad_generation/gpt2/checkpoint-39/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ad_generation/gpt2/checkpoint-39/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ad_generation/gpt2/checkpoint-39/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ad_generation/gpt2/checkpoint-30] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./output/ad_generation/gpt2/checkpoint-42\n",
      "Configuration saved in ./output/ad_generation/gpt2/checkpoint-42/config.json\n",
      "Model weights saved in ./output/ad_generation/gpt2/checkpoint-42/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ad_generation/gpt2/checkpoint-42/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ad_generation/gpt2/checkpoint-42/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ad_generation/gpt2/checkpoint-36] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./output/ad_generation/gpt2/checkpoint-45\n",
      "Configuration saved in ./output/ad_generation/gpt2/checkpoint-45/config.json\n",
      "Model weights saved in ./output/ad_generation/gpt2/checkpoint-45/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ad_generation/gpt2/checkpoint-45/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ad_generation/gpt2/checkpoint-45/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ad_generation/gpt2/checkpoint-39] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./output/ad_generation/gpt2/checkpoint-48\n",
      "Configuration saved in ./output/ad_generation/gpt2/checkpoint-48/config.json\n",
      "Model weights saved in ./output/ad_generation/gpt2/checkpoint-48/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ad_generation/gpt2/checkpoint-48/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ad_generation/gpt2/checkpoint-48/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ad_generation/gpt2/checkpoint-42] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./output/ad_generation/gpt2/checkpoint-51\n",
      "Configuration saved in ./output/ad_generation/gpt2/checkpoint-51/config.json\n",
      "Model weights saved in ./output/ad_generation/gpt2/checkpoint-51/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ad_generation/gpt2/checkpoint-51/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ad_generation/gpt2/checkpoint-51/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ad_generation/gpt2/checkpoint-45] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./output/ad_generation/gpt2/checkpoint-54\n",
      "Configuration saved in ./output/ad_generation/gpt2/checkpoint-54/config.json\n",
      "Model weights saved in ./output/ad_generation/gpt2/checkpoint-54/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ad_generation/gpt2/checkpoint-54/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ad_generation/gpt2/checkpoint-54/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ad_generation/gpt2/checkpoint-48] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./output/ad_generation/gpt2/checkpoint-57\n",
      "Configuration saved in ./output/ad_generation/gpt2/checkpoint-57/config.json\n",
      "Model weights saved in ./output/ad_generation/gpt2/checkpoint-57/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ad_generation/gpt2/checkpoint-57/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ad_generation/gpt2/checkpoint-57/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ad_generation/gpt2/checkpoint-51] due to args.save_total_limit\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 42\n",
      "  Batch size = 3\n",
      "Saving model checkpoint to ./output/ad_generation/gpt2/checkpoint-60\n",
      "Configuration saved in ./output/ad_generation/gpt2/checkpoint-60/config.json\n",
      "Model weights saved in ./output/ad_generation/gpt2/checkpoint-60/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ad_generation/gpt2/checkpoint-60/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ad_generation/gpt2/checkpoint-60/special_tokens_map.json\n",
      "Deleting older checkpoint [output/ad_generation/gpt2/checkpoint-54] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./output/ad_generation/gpt2/checkpoint-60 (score: 1.965510368347168).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=60, training_loss=16.426580810546874, metrics={'train_runtime': 567.8362, 'train_samples_per_second': 5.882, 'train_steps_per_second': 0.106, 'total_flos': 1300058505216000.0, 'train_loss': 16.426580810546874, 'epoch': 19.86})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tune GPT-2 model\n",
    "model_dir = f\"./output/ad_generation/{MODEL}\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Fine-tune GPT2 using Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_dir,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCHSIZE,\n",
    "    per_device_eval_batch_size=BATCHSIZE,\n",
    "    gradient_accumulation_steps=BATCH_UPDATE,\n",
    "    save_strategy=\"epoch\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    fp16_opt_level=APEX_OPT_LEVEL,\n",
    "    warmup_steps=WARMUP_STEPS,    \n",
    "    learning_rate=LR,\n",
    "    adam_epsilon=EPS,\n",
    "    weight_decay=WD,\n",
    "    seed=SEED,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,    \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    # compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output/ad_generation/gpt2\n",
      "Configuration saved in ./output/ad_generation/gpt2/config.json\n",
      "Model weights saved in ./output/ad_generation/gpt2/pytorch_model.bin\n",
      "tokenizer config file saved in ./output/ad_generation/gpt2/tokenizer_config.json\n",
      "Special tokens file saved in ./output/ad_generation/gpt2/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advertisement generation using the fine-tuned model\n",
    "\n",
    "Now, we can generate ad contents using fine-tuned GPT-2 model. To evaluate the \"goodness\" of the ad generation model, we select the first sample in the validation set as an example and compare the generated job contents with the real content.\n",
    "\n",
    "### Load tokenizer and fine-tuned model\n",
    "\n",
    "First, load the saved tokenizer and trained fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file ./output/ad_generation/gpt2/vocab.json\n",
      "loading file ./output/ad_generation/gpt2/merges.txt\n",
      "loading file ./output/ad_generation/gpt2/tokenizer.json\n",
      "loading file ./output/ad_generation/gpt2/added_tokens.json\n",
      "loading file ./output/ad_generation/gpt2/special_tokens_map.json\n",
      "loading file ./output/ad_generation/gpt2/tokenizer_config.json\n",
      "loading configuration file https://huggingface.co/gpt2/resolve/main/config.json from cache at /home/hpan/.cache/huggingface/transformers/fc674cd6907b4c9e933cb42d67662436b89fa9540a1f40d7c919d0109289ad01.7d2e0efa5ca20cef4fb199382111e9d3ad96fd77b849e1d4bed13a66e1336f51\n",
      "Model config GPT2Config {\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50257,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50258,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"pad_token_id\": 50260,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"sep_token_id\": 50261,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": {\n",
      "    \"text-generation\": {\n",
      "      \"do_sample\": true,\n",
      "      \"max_length\": 50\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/gpt2/resolve/main/pytorch_model.bin from cache at /home/hpan/.cache/huggingface/transformers/752929ace039baa8ef70fe21cdf9ab9445773d20e733cf693d667982e210837e.323c769945a351daa25546176f8208b3004b6f563438a7603e7932bae9025925\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and saved model\n",
    "tokenizer = get_tokenizer(load_token_path=model_dir)\n",
    "model = get_model(tokenizer, special_tokens=SPECIAL_TOKENS, load_model_path=os.path.join(model_dir, 'pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate prompt for ad generation\n",
    "\n",
    "Generate the prompt for ad generator by joining the ad title and ad abstract from the first sample in validation set into the prompt text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|BOS|>Pricing Analyst's all levels, Sydney CBD<|SEP|>Superb roles with leading Financial Services firm. Combine Actuarial/Pricing Analytics with the latest Data Science, Machine Learning techniques<|SEP|>SQL;SAS;Python;ability in the fullness of time to suggest process improvements as well as the quality of your stakeholder relationships;working as part of a customer-facing team who provide internal consultancy to the business, supporting the execution of various strategies from a pricing perspective<|SEP|>\n"
     ]
    }
   ],
   "source": [
    "# choose the first sample in test dataset for testing\n",
    "title = test_df.iloc[1]['title']\n",
    "abstract = test_df.iloc[1]['abstract']\n",
    "keywords = test_df.iloc[1]['keywords']\n",
    "kw = ADDataset.join_keywords(keywords, randomize=False)\n",
    "\n",
    "prompt = SPECIAL_TOKENS['bos_token'] + title + \\\n",
    "         SPECIAL_TOKENS['sep_token'] + abstract + \\\n",
    "         SPECIAL_TOKENS['sep_token'] + kw + SPECIAL_TOKENS['sep_token']\n",
    "\n",
    "        \n",
    "# # prompt = SPECIAL_TOKENS['bos_token'] + title + \\\n",
    "# #          SPECIAL_TOKENS['sep_token'] + abstract + SPECIAL_TOKENS['sep_token']\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "display the real advertisement content of the example case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' My client is a Financial Services organisation who continue to go from strength to strength with their innovative product suite and good governance enabling them to continue growing as we move into 201.With a nascent new Product Pricing Team growing currently they are looking for talented and experienced Actuarial / Pricing Analyst.s to join their team. I am looking for applications from candidates with strong mathematically orientated academics  BSc, MSc etc in a Maths focused Degree . e.g. Economics, Statistics, Mathematics, Actuarial Studies, Operations Research et al and at least 12m ideally 3 years but more experienced candidates are also encouraged to apply experience in an Analytically focused role within a General Insurance team. In this role you will be working as part of a customer-facing team who provide internal consultancy to the business, supporting the execution of various strategies from a pricing perspective. With numerous initiatives underway your role will require a high level of technical competency  fluent programming /modelling skills/capabilities from a traditional tech . e.g. SQL, SAS, Emblem etc.- .and more .data science. orientated perspective . Python, R etc. as you will be involved building Actuarial and Statistical Models for innovations and solutions in the pricing arena. GLM/Emblem experience would also be well regarded. Candidates also need a solid understanding of the GI market locally including products . services for my client as well as having a decent grasp of the competitor landscape. This is an innovative Pricing team which works pretty much end to end in comparison with traditional pricing teams and they are continually looking to new technology/techniques e.g. Machine Learning techniques, AI, Data Science/Big Data approaches in order to innovate from a Pricing perspective and deliver a competitive advantage to the business. .. This is an excellent role within a team with a strong and supportive culture. This is a high performance team where what you deliver will be measured along with your ability in the fullness of time to suggest process improvements as well as the quality of your stakeholder relationships. With strong leadership which is well regarded in the industry this is an exciting time to be joining this function. With multiple roles available for the right candidate these opportunities offer successful applicants a very real chance to enjoy meaningful career progression over the next 2-5 years. The team is highly visible within the business where excellent work will get recognition from key people from a Senior Management level. .. In short, these roles offer some of the best Pricing/Actuarial Analytical roles you are likely to find in Sydney this year. Superb opportunities. Interested candidates should apply ASAP with an up to date CV Word Format ideally to declan.oboyle..com.au or apply via the online facility or call Declan on 0452 411 968 for a confidential discussion. '"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = test_df.iloc[1]['clean_text']\n",
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the prompt text into vectors using the tokenizer and evaluate the generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50262, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50262, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "# Run model on GPU.\n",
    "device = torch.device(\"cuda\")\n",
    "generated = generated.to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad generation using greedy search\n",
    "\n",
    "Generate the ad contents using the greedy search and return 10 samples with different criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: eople are constantly changing their daily lives you will need both analytical skills such analysis capability knowledge at AIM level. You may also be required by other regulatory agencies including FIFO Regulation Authority Australia,.to complete this role please apply online through our website \n",
      "\n",
      "\n",
      "2: years experience working within Australia based sales channels / platforms such eCommerce systems using SQL Azure databases Ability +3+years market intelligence skills Excellent understanding about complex data pipelines including RDF Analysis Experience building predictive models A high level analytical approach which helps you understand how customers use products effectively What they do Best practice when faced With multiple options available To work autonomously across large quantities depending upon user needs Flexible hours You may choose between two different kinds : 1) Working under pressure 2............a lot more flexible Hours per Week $10 hrs plus bonus if required Location..to attend training sessions Monday through Friday?????s preferred??hrs only! Please note however please don't contact me directly regarding any technical issues during recruitment via our website but we appreciate it!. If there isnt enough interest then apply here For further information visit www.;www.,com.-au,.org/. We would like every applicant contacted by phone 24 days forward worded resume ASAP so make sure everything goes smoothly!! About us... As mentioned above Our focus has always been creating great company values while retaining ownership over those unique qualities being nurtured globally By hiring someone capable I am seeking somebody willing too eager not just inquisitive minds But energetic individuals driven intellectually Strong communication Skills Must have strong interpersonal styles Confident rapport Extensive knowledge Word processing Vocational development Previous industry exposure Preferred qualifications Keywords Marketing & communications First Aid etc Competitive salaries range around AUD$200K-$400k ($700KS+) Only minimum 6 months' paid contract Duration 12 month salary Bonus opportunity??? Apply now NOW FOR MORE INFORMATION CLICK THE 'Apply Now', FIND YOUR REQUIREMENTS ONLINE OR CALL TEN CREDIT SITE TO APPLY SOLELY AND EXPERIENCE WHILE SHUTTLED IN IT! Do YOU want something new? Want some exciting opportunities lined up today???????? No worries...We can accommodate everyone regardless why others aren`re interested In doing what makes sense Business success depends heavily On people giving back Some say very few good things Others Say many Good Things Are successful At times these statements feel true However sometimes conflicting personal stories are best kept private! Join Hands Free Chat Weekly email address xxxxxx@xxxxxxxxxxx Phone number *required Click Here Be careful!!! Send CV out quickly after applying online preferably before Christmas Day!! The Responsibilities Your job requires Desire Have fun growing into Manage daily tasks Develop monthly reports Analyse client requirements Perform periodic emails Work closely alongside clients Establish regular updates Check social media accounts Ensure accurate reporting includes tracking users profile usage Conduct surveys Assist Customer Care Coordinate product maintenance Provide support services Provide assistance relating specifically related matters Contribute especialty advice Deliver marketing content Indemnify applicants Respond verbally Attach tape recording material Support employees Help maintain order management Prepare promotional materials Produce copyable documents Present written response forms Participate independently Identical duties Pay close attention Children Friendly Company Applicants MUST HAVE NO IDENTITY WITH REGISTERED AGENCIES THAT ACTUALLY PRINTING BUSINESS ENROLLMENT SERVICES ARE AT\n",
      "\n",
      "\n",
      "3: on fund is currently offering an annual bonus package up front through our new headquarters located at 41 Waverley Drive SuiteFIFO! We pride ourselves not only providing excellent value but also delivering great service across multiple channels via communications capability that enable us maximise profitability while ensuring we deliver maximum results throughout this exciting period!. For further information about how you can join or get involved please visit: www.;www., \n",
      "\n",
      "\n",
      "4:  they give themselves To ensure that we are getting through our sales call quickly after closing date however if this sounds like something out there please apply via PMs..com.au..or email me directly! www,.linkedin... For further information visit :www&irc....tmsn..............#Finance Finance | LinkedIn We're looking forward working closely alongside Australia Strong client service experience combined With strong AI capability provided by experienced CPA & SQL specialists I am highly motivated when it comes up against challenging technical challenges so be careful what works best FORMAT DATA COLLECTORS AND EXPERIENCE FINDINGS ABOUT THE ROLE OF PRIVATE AGENCIES? You may want some background knowledge about finance within Business Intelligence econometric framework iCal Dynamics, Macroeconomics Analytical Statistical Models Proven Applications Skillset Flexible work life balance management Experience building scalable databases ECONOMIC CREDIT REQUIRED CONSULTANT BENEFITS RATES PER PERSON ASKED APPLY NOW OR CALL JORDAN ON 02 5536 560 855 679 Alternatively contact Matt Larkin today xxxxxxxx@bankline bankingonlineforum.,co...en espanosales!.org +61 528 7426 3227 Or read below About us First off BankLINE is committed always being innovative but has never been shy around competition Our focus focuses heavily towards providing high performing clients across multiple platforms using proven technology solutions leveraging industry standards Excellent communication ability Great interpersonal engagement Good understandingof current market trends Competitive salary packages Effective competitive commission structure Opportunity opportunities High calibre employees Highly paid staff What Are They LookingFor If someone doesn't sound familiar feel free notify matt lennon2367 robertshire 020 79270 9369 jeremy c o'clock joie mccarthy robinson combservice en route \n",
      "\n",
      "\n",
      "5:  how best through an iterative approach / plan A thorough understanding that leads up front solutions based on real world events like sales volumes etc.. Knowledge about Microsoft Dynamics BI tools such BigQuery Workflow Tools. You'll also have access too!. If this sounds similar please apply via our contact form below!  About us : Our purpose are seeking experienced RPA specialists into Australia so we can offer cutting edge technology development services while providing exceptional service throughout its lifetime at market rates without compromising customers' personalised experiences within it.. We value honest discussion between people committed intellectually rather than verbally Engagement oriented At least 2 years Australian Police Service background Check current LinkedIn profile Please note if using any other company names above,.com | www xkcdarellabearly?saleservice=en&explanation|companyName Role Summary Salary Bonus Opportunity Qualifications Desirable Skills Excellent communication abilities Ability To work independently Ideally able Have strong analytical capability Strong written & verbal writing Fluent problem solving Expertise Analytical thinking Effective management systems Solid interpersonal rapport Good communicator Great interest level 3+ Cert III qualification 5% commission $200K per month Minimum 12 months police clearance Current passport Quantity First Aid kit Must include CPR card Confident Communication Abilities Able Lives Well developed personality Essential Health Insurance Planner What would I look forward do??????????????............and more??...???? Applicants must submit their CV by Monday 6th May 2019!! For further information visit jennifer@cafetechworksonlineinvertebratebankormenta espaol.;www JENIOR ENROLLMENT POLICY PLEASE NOTE THAT NO CATERING OR PRIVACY INFORMATION COLLECTED IN THE OFFICE CAN BE FURTHER DISQUIRED AS APPLIES TO APPLICANTS WHO ARE AGREEMENT DUTYING THEIR EXPERIENCE AND DO NOT HAVE VISA LITERALLY REQUIREMENTS FOR THESE ACTUaries WE REGISTER AT OUR SOLEEST EXTENT OF SERVICE WITHIN RESPONSIBILITIES ONLINE WHILE WORKNING FULL TOURS ABOUT YOU!!!! Your successful applicant MUST hold themselves out against fraud when conducting interviews under these conditions during employment hours where they may face criminal prosecution over conduct deemed offensive towards them By submitting online applications only applicants should consent prior receipt before being contacted directly regarding what has transpired whilst performing activities related tasks which require immediate attention Due Date Selection Criteria Apply NOW!!! As always feel free send emails quoting references off eCommerce websites listed here...please keep everything confidential until acceptance email confirmation provided after application processing date! Thankyou Joe M., Regis Park South Brisbane, NSW 761 9270 2326 EAPLES CALL FIRST CLASS SHOP Friday 13 June 1 8am Thursday 1300 5328 4227 Phone No.: 0445220967 Or call 0740 79554829 Alternatively join Joining Group chatroom On 02nd July 2017 20219 0023 Email enquiries welcome inquirescanfallbacker1016ticketsforjohannescolerebello pr ompassion en garde fr\n",
      "\n",
      "\n",
      "6: rs????????????s own teams? The key areas we look forward into include providing insights about how consumers connect directly or indirectly across brands - whether they use their phones during shopping trips / using online banking services via SMS etc.. Key initiatives would focus specifically at developing high performing Customer Experience Content delivery systems like mobile apps. Strongly developed analytical frameworks involving real world applications analysis tools designed primarily around Sales Intelligence capability Analytical modelling tool development Skills Development & improvement In this respect Australian PPCA offers highly regarded Senior Accountant positions available both privately owned offices overseas Friendly Liaising Team memberships offer excellent opportunities which allow one person access +1 x 1 Contact details please call 0420 828 9902 quoting company number below : 03 02 9227 If interested send email contact jennifermcraigonford...to get more information visit www!completionplanningcompanies!.org&referring.. Please note our current listing does have shortlisted candidates submitting resumes before 6pm ET Monday 19 May 2019 due no change yet!! We encourage applicants presenting themselves verbally if applicable while reading some detail relevant documentation provided by industry experts.... About us You can find out why I am currently recruiting here At Australia Bank Holdings Ltd,. Located just off Northfield Road between Barton Park Drive SW NSW 12203,the bank has been expanding its presence throughout recent years seeking innovative new markets looking after customers globally over many different industries worldwide...... Our aim was always growing fast whilst offering unparalleled value proposition driven growth strategy without sacrificing profitability By closing today there won't ever come any disappointment.....You'll need either ocassionate commercial acumen (strong communication) scoped knowledge?? Have good sense........or something similar??? To apply click up above SEE MORE NOW AT APPLY TO BE ADVISED ABOUT OUR DIVERSITY OF SERVICE FOR YOURSES AND FUTURE ROLE CREDIT SITE Click Here For More Information On Why Apply Now | What Do Some Of These Be Like Or Are They Looking ForwardTo Relevant Job Seekers Who Will Interest Us CLICK THE SHARE button next link menu image Search Results With Only \n",
      "\n",
      "\n",
      "7: nology trends 3..experience dealing autonomously around complex algorithms using AI 5..knowledge regarding client performance analysis tools 6.a high level grasp on analytics or predictive modelling suchas Amazon Web services 8 Experience trading futures contracts / commodities Trading through FXX Markets Expertise managing key financial institutions Including but not limited To assist you understand how these technologies work Provide detailed information explaining why they do what their software does What kinds III& IV Options available If this sounds like something someone out there needs then click here! We're looking forwardto finding more jobs at Ritz Carlton!! About The Role As our Senior Business Managers we represent over 50 countries worldwide providing exceptional service by delivering innovative solutions both domestically owned products via international sales channels Our purpose has always been driven largely mainly against equity interest rather than competition Interested applicants must be willing consent required Proof omitting any references above may require contacting us directly ASAP Please note however no confidential communication would necessarily help save face...or make it better!. Apply now today if interested please contact me jennifer@referralfolioprojectionservice dot com : echat +61 202 201 9227 xxxxxxxx | www+mailbox...commentedexpert??xxxxxxxxxxx;Fluent English proficiency Knowledgeable French Language Fluent Skills Excellent writing Ability Computer literacy Good written & verbal development Proven numeracy You should possess some common sense qualifications Qualifications Desirable qualification APA Cert II Current Police Check Card Essential Previous police check papers Competency Test positive Vocational Preparation Work ethic First Aid Certificate Licence Australian Citizen Permanent Resident Citizenship Visa International Residency Immunisation Statement Training In relation With Children For further discussion visit http://wwwircvoterproficiencyonline,.org?rmp=1#faq Alternatively call Sarah J Campbell (613 74232 quoting code) 24 7135 02098 Email enquiries into vacancies apply online Thursday 04 May 2019 11am Eastern Time Phone 03 5523 5326 email inquires back Friday 02 June 19th Monday 017203 738070 This message includes personal details only Your Name *required field Description Have I contacted somebody else recently yet????? No Thanks? Yes Thankyou Very friendly reception thank yous!!!\n",
      "This job advertisement requires immediate acceptance NO FURTHER APPLICANTS ARE PRIVATE AND MUST BE AGREED TO HAVE LEGAL ENROLLMENT DIFFERENCES REQUIRED FOR THE OFFICE NOMINATION NOTICE OF EXTRAORDIAL BENEFITS IN VISA REGISTERING POLICY MATERIAM LITERARY NOTE ABOUT THESE INFORMATION EXPRESSLY DISCLAIM ALL WARRANTIES OR CONDITIONS THAT OUR ENSULTIMALLY RESPONSIBLENESS CAN CAUSE PERSONNEL INTEREST WITH US WHILE WE WORK ON COLLECTIVE COMMITTEE SERVICES............PROCEEDINGS AT AIRPORT LOCATIONS INCLUDION PARK SHOP ASKMENTS HOW YOU MAKE SURE ITS CLEARANCE IS TAKENTATIVE NOT ACCEPTANT COMPLIANCE CUTTING ASSISTIVES SKILL LEVEL SUPPORT GU\n",
      "\n",
      "\n",
      "8: a pipelines etc.;Understanding how market processes work eg client orders coming into commission Agile strategy that aligns best interest criteria..with understanding what you are doing The ability at times not only enhances performance but also promotes trust between suppliers As this opportunity comes along it can be advantageous..to have someone else join their staff so they may take responsibility?. At least until there isn't any doubt about where our money goes... This role will need some experience covering complex digital platforms like IoT applications e., cloud services AWS Hadoop technologies EPDH systems Cloud based application development tools Azure SQL scripting R programming Tools used include Git 2 + Django 3+ Python 5 plus Tensorflow 8 GIT Pro version available  Our clients come across many different approaches when looking after large volumes????????????ll focus mostly around products offering real value?? About us We're passionate people committed enough never let others down!! What we offer You get paid hourly rate $200 per hour! All day long hours Monday  Friday 9am till midnight ET Only minimum 12pm shifts Weekly Bonus Planner Job Offer x3 Opportunities Apply now NOW via link below Send email address : info@tosalesonlinebankers Australia please note these benefits apply regardless if I am applying online,. If successful then feel free send me two emails quoting job listing number, phone 23408 5523 jennifer robinson laura vincent scott walsh katherine johnson paul dolan matthew roger mccarthy c o'Brien s taylor gordon wwwwww............com | LinkedIn http://bitlyricschallenge!.mpg...and more Facebook Group My resume MUST meet one other than mine above * Required Minimum 6 months Business acumen Excellent communication Skillset Strong organisational discipline Flexible career progression Adaptable personality Check out Joining MSX today For further information visit https:/twitter&familiarity=en Here next month opportunities open up throughout May 2019 To find jobs click here Have questions ABOUT THE ROLE OF ENROLLING OUR BUSINESS? Please submit them directly contact Tom Murray (TMR) 01 02 03 04 05 07 026 202 722 1300 ext 2309 superannuation fund company admin webmaster telesys com office chat room adelaide dot essex abc gymnsworld australia david christopher martyn nicolson enviorn bobbie jackman nick mcintosh cdm priscus callum jefferson joanne kevin r JOIN US FOR MORE INFORMATION ON HOW TO MAKE SURE YOU ARE DIFFERENT IN YOUR SKILLES AND APPLY PLEASE CALL LIVESTREAM AT 201 7319 560 Fax ---------- Previous Next First name last names Email Address ------ Current Role Description CPA Competitive Accountant Client Responsibilities Confident Work Life Cycle Full Time Employment Interest Free Cash Flow Insurance Working With Children Benefits Tax Assistance Support Team Friendly Employee Recruitment Unit Care Manager Casual Relationship Planning International Contact Us Search Your Profile Click 'Apply Now', choose either Option 1 CLICK YES button OR select ''YES'' NO OFFER AGREEMENT APPLICATION RE\n",
      "\n",
      "\n",
      "9: s using data science tools like SPAG Insight Analyse?????& Spark SQL databases are available online. You can also join our Customer Support Team at www..composite..or call +61 02 9227 201... For further information please contact Adam Naylor on 0426 592 8270 (with references) via email enquiries@referringcompanies...and be sure that you have provided accurate commercial documents so we may receive this communication without prior consent! About us We currently hold offices located around Melbourne Beach NSW covering areas ranging between Commercial Drive South Street North Shore Road West End Canberra City Airport airport Brisbane Port Macquarie International Rd East Coast Retail Centre Market Place Farmington Park Waterford Gardens Perth Campus Keyfield office space Maple Field area Location??9268 E Main St Suite The role requires experience working alongside highly regarded financial services analysts looking after their clients internationally Working closely during regular trading hours Weekly shifts 2pm Eastern Time Friday hrs Monday 4am till closing Hours 3 days per week Sunday 7 pm until close Please note however some vacancies will only open up if they meet criteria outlined above by Relevant Federal Government agencies eg Citizenship Check No more than one month required Experience applying directly under contract To apply today visit ompaonline.,govt,.edu website | Apply NOW!. This position has two pages listed below which cover everything I've ever done before moving onto what it takes............to get here!! What would make life better? Good job doing something amazing now?? Why do people Do whatever makes sense If there isn't any problem then why bother seeking help? Aspiring CPDs want strong leadership abilities Strong analytical thinking Ability balance Able enough knowledge about technology etc Excellent interpersonal styles Have excellent written writing Skills able understand complex issues Understanding systems architecture Effective use cases Extensive background Knowledgeable understanding Computer literacy Familiarity building good oral communications Communication Diploma Requirements Bachelor III Minimum 0 1 year postgraduate education Required APPLY ASAP PLEASE NOTE THAT WE DO NOT MAKE REQUIRED APPLICENCES FOR THE POSITION WHO IS ENROLLED TO SEEK CANNABIS OFFICE ADOPTION NOTICE IN RIGHTS OF PERSONAL DATA COLLECTION AND EXAMINATION INFORMATION ABOUT OUR BUSINESS INDUSTRY ARE MADE AT NO POINT ON THIS DISCUSSIONS POLICING PAGE OR ITS REGISTER AGREEMENT SOLELY AS YOU FURTHER KNOW HOW THESE PAGES WORK By submitting these econometric queries outbound telephone applications confidential personal info must include both verbal greeting cards matching name card number SSN code MSX preferred bank account details phone bookmarked copy document format SMS attachment Formatting metadata relevant documentary evidence pertinent material relating specifically related Business Intelligence Unit Essential Qualifications Applicants MUST BE USER ACTIVATED WITH VISA BINDINGS PERMITTEN BY THIRD PARTIES WHILE WAIVERS HAVE IMPORTANT BENEFITS IT WOULD ALSO Be An Agile Opportunity Provide honest advice where appropriate Conducted surveys Proactively communicate current affairs With other qualified candidates Deliver timely updates On application form attach police\n",
      "\n",
      "\n",
      "10: consulting directly at client end customers through an experienced financial adviser using Microsoft Excel tools such AsyncX + GIT toolbox etc.;providing input into complex technical issues within existing projects while also helping solve problems created by those solutions via statistical modelling methods????????????solving these challenges in conjunction With this opportunity being provided only recently but it looks like there may soon have been some real interest out here! If everything goes smoothly then we can expect our superannuation fund manager Peter McQuaid Jr., Tom Watson III & others..to join us!! To apply please click 'apply' below,. This job has no cover letter required so contact Adam Blackburn upon 1300 696345 if interested!. Do not worry about what they think because I?tll know more............we would love every single one??for applicants looking forward too much work...just want someone else willing&reager enough......a bit creative!!! A few days back We currently hold two vacancies available : APPLY NOW OR CALL FOR MORE INFORMATION ABOUT HOW TO MAKE SHORTEST PRIVACY POLICIES SO THAT WE CAN BENEFITS YOU AND THE TEAM By submitting confidential applications above however PLEASE NOTE OUR EXPERIENCE IS NO LONGER DISQUALIFYING YOUR CURRENT AGENCES IN ITS REQUIREMENTS OF POSITION ENROLLMENT PERIOD WITHIN 3 MONths NOTICE MUST BE MADE FIRST REGISTERED ONLINE AT wwwirclequakechatdbgfaqnvmpccfjhclauxtrngxcomwwwusesaleservice...and preferably ASAP after applying online You must submit proof positive documentation relevant specifically relating themselves successfully under penalty fide qualifications which includes Proof Check IIIIIII IV V ECONOMICS ITEMS For further information regarding any other vacancy check up Apply now Please note current hours Monday 8am till midnight Thursday 7pm until closing Friday 11 am.-Saturday 9 pm 2 hrs before Christmas Hours Only Applicants eligible should make prelisted phone calls during regular shifts when possible e xperience welcomeTo meet Australia residents visit http://ausparkshopfaironlineservicesorcountygovularies/.examples Here At Our Headquartered located right next door NSW City Centre Phone 061 5998 201 Fax 0420 5606 Ext.: 02 25 5527 Alternatively email jennifermcquesse@gmail especialtylifemanagement dot com About me Who loves learning Languages English Strongly Agile People Experience Successful Lives Life Cycle Work Out Skills Personal Growth Interested In Working Desirable Career Pathologies Good Health Desire Have Fun Enjoy Your Company Fantastic Job Seekers Are On Wellbeing Role Models Mentally Ill Friendly Community Managers Essential Resources Excellent Fit Supportive Team Culture Positive Values Inspire Highly Effective Self Help Program Responsible Leadership Opportunities Big Brother Family Great Benefits Huge Rewards Awesome Opportunity Able Business Liaise Welcome Back Home Free Chat Live chatroom\n",
      "Join Us Join Me Now FREE TIP OFF ASKING QUESTIONS PICT\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Top-p (nucleus) text generation (10 samples):\n",
    "sample_outputs = model.generate(generated, \n",
    "                                do_sample=True,   \n",
    "                                min_length=50, \n",
    "                                max_length=MAXLEN,\n",
    "                                top_k=30,                                 \n",
    "                                top_p=0.7,        \n",
    "                                temperature=0.9,\n",
    "                                repetition_penalty=2.0,\n",
    "                                num_return_sequences=10\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    text = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "    a = len(title) + len(abstract) + len(','.join(keywords))\n",
    "    # a = len(title) + len(abstract)\n",
    "\n",
    "    print(\"{}: {}\\n\\n\".format(i+1,  text[a:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ad generation using beam search\n",
    "\n",
    "Generate the ad contents using the beam search and return 5 samples with different criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/fred/oz144/anaconda/envs/nlp/lib/python3.7/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  ../aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: t trends using analysis tools including ML or R;Analyse trading patterns using quantitative modelling methods such as Bayes' Uncertainty Hypothesis Estimator;Manage intra-annuation remuneration for accounts receivables based on interest rates;Provide technical assistance when required due to client needs;Work autonomously at critical junctures during periods of high volume - particularly times of large volumes We are looking for experienced financial services analysts willing to join our team.. If this sounds like you, please apply today! ??????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????????\n",
      "\n",
      "\n",
      "2: ness Case Studies, Risk Analysis, User Experiences, Confidence Estimations, Predictive Forecasting, Manage Leverage & Indemnify Your Audience. You will be required to have demonstrated experience working autonomously within Microsoft SQL Server 2012 R2 or newer using highly regarded tools such as Sqlite, MariaDB, Pandas, Entity Framework 3.5, Hibernate, Jupyter Notebook, Tableau, Elastic MapReduce, ETL, Tensorflow, Gulp, Spark, Kubernetes, Kafka, AWS Lambda, Redshift, Restful Native App Engine (RNG) / Big Data Experience - Desirable Qualifications  What we're looking for: An Accountant capable of delivering high level technical analysis on complex issues such as Customer Relationship Management, Business Case Studies, Risk Analysis, User Experiences, Confidence Estimations, Predictive Forecasting, Manage Leverage & Indemnify Your Audience. Ability to identify and prioritize opportunities for improvement across multiple stakeholders including: customers, suppliers, clients etc.. We are looking for an experienced Accountant that is able to deliver high level technical analysis on complex issues such as Customer Relationship Management, Business Case Studies, Risk Analysis, User Experiences, Confidence Estimations, Predictive Forecasting, Manage Leverage & Indemnify Your Audience If you want to apply now please submit your CV via our contact form at jennifer.mccarthy@microsoft.com +61 02 926 5520, email jennifer.mccarthy[at]gmail.com For further information about this position visit www.linkedin.com/company/jennifer mccarthy?q=en&fref=hg&sigid=40192823221216 About This Role The role????????t looks like it would suit any kind of person! I am currently developing a brand new character based on JoJo's Bizarre Adventure manga artist\n",
      "\n",
      "\n",
      "3: P. Morgan, BNP Paribas, Mediapartner etc., but has also been used on behalf of major drug companies like GlaxoSmithKline, Eli Lilly, AstraZeneca, Bristol-Myers Squibb and others. \n",
      "\n",
      "\n",
      "4: se large quantities of data using multiple dimensional systemsAbility to work autonomously within an organisational environment - particularly if you are working on complex projects like Customer Relationship Management / Establishing Relationships between Customers and Providers \n",
      "\n",
      "\n",
      "5: reaves Lansdown, Gartner & Co.;Fitzgerald Pty Ltd., Fintech Group, KPMG&Co.;Conducted an extensive review of key performance indicators for Australian companies during the last 12 months looking at how they responded to market shifts such as interest rates, stock price movements, growth trajectories or other external factors that have led to significant changes in company operating results..The successful candidate will possess: Ability to work autonomously across multiple stakeholders and be able to make strategic decisions on behalf of both client and management Experience working autonomously across multiple stakeholders and be able to make strategic decisions on behalf of both client and management A proven track record of delivering high performing projects using cutting-edge technology Demonstrated ability to translate complex data into actionable insights What we're looking for: An experienced quantitative analyst capable of translating complex data into actionable insights About this role: \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Beam-search text generation:\n",
    "sample_outputs = model.generate(generated, \n",
    "                                do_sample=True,   \n",
    "                                max_length=MAXLEN,                                                      \n",
    "                                num_beams=5,\n",
    "                                repetition_penalty=5.0,\n",
    "                                early_stopping=True,      \n",
    "                                num_return_sequences=5\n",
    "                                )\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    text = tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "    a = len(title) + len(abstract) + len(','.join(keywords))\n",
    "    # a = len(title) + len(abstract)    \n",
    "    print(\"{}: {}\\n\\n\".format(i+1,  text[a:]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1800cb1e82eaed827e3351919b3468e9b01564a029e9408d7d44a2e0754d9b63"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit ('nlp': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
